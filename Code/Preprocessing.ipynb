{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03e601b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363fba9a",
   "metadata": {},
   "source": [
    "Environment Setup\n",
    "1. create an isolated env \n",
    "\n",
    "conda create -n tinygesture python=3.10 -y \n",
    "conda activate tinygesture\n",
    "\n",
    "2. PyTorch + CUDA 12.1 (change to 11.8 if your driver is older) \n",
    "\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "\n",
    "3. Jupyter + misc libs \n",
    "\n",
    "pip install notebook numpy scipy matplotlib pillow tqdm\n",
    "\n",
    "4. pip install --upgrade pip wheel setuptools\n",
    "\n",
    "5. PyTorch + CUDA\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2164558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple, List, Dict, Callable, Optional\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "import argparse, textwrap\n",
    "__all__ = [\n",
    "    \"RadarDataset\",\n",
    "    \"build_dataloaders\",\n",
    "]\n",
    "import platform, sys\n",
    "print(torch.cuda.is_available(), torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0873dd",
   "metadata": {},
   "source": [
    "TinyRadarNN gesture‑dataset loader (supports .txt, .npy, .png)\n",
    "\n",
    "Directory layout expected\n",
    "-------------------------\n",
    "```\n",
    "Datasets/\n",
    "    5G \\ data/\n",
    "        FingerSlider/\n",
    "            session_0/\n",
    "                0_FingerSlider.txt\n",
    "                1_FingerSlider.txt\n",
    "                ...\n",
    "            session_1/\n",
    "        PalmTilt/\n",
    "        ...\n",
    "```\n",
    "\n",
    "Each **clip** is stored in a text file where whitespace or comma separates floating‑point numbers laid out row‑wise (first row = sweep‑0, etc.).  The loader will infer the number of time‑steps *T* from the file length and an optional `range_bins` argument.\n",
    "\n",
    "If you have pre‑converted `.npy` tensors or image‑encoded spectrograms (`.png`), set `file_ext=\"npy\"` or `file_ext=\"png\"` in the constructor; everything else stays the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c64b2a",
   "metadata": {},
   "source": [
    "# RadarDatset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ca30484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "NUMERIC_START = tuple(\"0123456789-+.\")\n",
    "\n",
    "\n",
    "def _numeric_row(line: str) -> Optional[List[float]]:\n",
    "    \"\"\"Return list of floats if the line starts with a numeric token else None.\"\"\"\n",
    "    ls = line.strip()\n",
    "    if not ls or ls[0] not in NUMERIC_START:\n",
    "        return None\n",
    "    try:\n",
    "        return [float(tok) for tok in ls.split()]\n",
    "    except ValueError:\n",
    "        return None  # non‑numeric token – treat as header\n",
    "\n",
    "\n",
    "def _load_clip_txt(path: Path, range_bins: int) -> torch.Tensor:\n",
    "    \"\"\"Load a .txt clip → (T, R) float32 tensor, ignore arbitrary headers.\"\"\"\n",
    "    rows: List[List[float]] = []\n",
    "    with path.open(\"r\") as fh:\n",
    "        for ln in fh:\n",
    "            row = _numeric_row(ln)\n",
    "            if row is not None:\n",
    "                rows.append(row)\n",
    "    if not rows:\n",
    "        #raise ValueError(f\"{path}: empty or no numeric data found\")\n",
    "        return None\n",
    "\n",
    "    mat = np.asarray(rows, dtype=np.float32)  # (T, R?)\n",
    "    if mat.shape[1] != range_bins:\n",
    "        raise ValueError(\n",
    "            f\"{path.name}: expected {range_bins} range‑bins, got {mat.shape[1]}\"\n",
    "        )\n",
    "    return torch.from_numpy(mat)  # (T, R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33fec6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarDataset(Dataset):\n",
    "    \"\"\"TinyRadarNN gesture dataset (txt / npy / png).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: os.PathLike,\n",
    "        gesture_names: Sequence[str],\n",
    "        *,\n",
    "        range_bins: int = 64,\n",
    "        file_type: str | None = None,\n",
    "    ) -> None:\n",
    "        self.root = Path(root)\n",
    "        self.gesture_names = list(gesture_names)\n",
    "        self.g2idx = {g: i for i, g in enumerate(self.gesture_names)}\n",
    "        self.range_bins = range_bins\n",
    "\n",
    "        self.file_type = file_type  # \"txt\", \"npy\", \"png\" or None → auto\n",
    "        self.index: List[Tuple[Path, int]] = []  # (path, label)\n",
    "        self._build_index()\n",
    "\n",
    "    # ------------------------------------------------------------------ index\n",
    "    def _build_index(self) -> None:\n",
    "        \"\"\"Populate `self.index` with (Path, label) tuples, skipping bad clips.\"\"\"\n",
    "        exts = {\".txt\", \".npy\", \".png\"}\n",
    "\n",
    "        for g in self.gesture_names:\n",
    "            # Primary path: <root>/<gesture>/**            (5G layout)\n",
    "            g_dir = self.root / g\n",
    "            if g_dir.is_dir():\n",
    "                candidates = g_dir.rglob(\"*\")\n",
    "            else:  # fallback deep search (11G layouts)\n",
    "                candidates = self.root.rglob(f\"**/{g}/**/*\")\n",
    "\n",
    "            for p in candidates:\n",
    "                if not p.is_file() or p.suffix.lower() not in exts:\n",
    "                    continue\n",
    "\n",
    "                # Skip empty .txt clips right here to avoid runtime crashes\n",
    "                if p.suffix.lower() == \".txt\":\n",
    "                    if _load_clip_txt(p, self.range_bins) is None:\n",
    "                        continue  # no numeric data → ignore clip\n",
    "\n",
    "                self.index.append((p, self.g2idx[g]))\n",
    "\n",
    "        if not self.index:\n",
    "            raise FileNotFoundError(f\"No clips found under {self.root}\")\n",
    "\n",
    "        # Auto‑detect file type if user did not specify one\n",
    "        self.file_type = self.file_type or self.index[0][0].suffix[1:]  # drop dot\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path, label = self.index[i]\n",
    "        if self.file_type == \"txt\":\n",
    "            x = _load_clip_txt(path, self.range_bins)\n",
    "        elif self.file_type == \"npy\":\n",
    "            x = torch.from_numpy(np.load(path)).float()\n",
    "        elif self.file_type == \"png\":\n",
    "            import PIL.Image as Image\n",
    "            x = torch.from_numpy(np.asarray(Image.open(path)).astype(np.float32))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file_type {self.file_type}\")\n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bf054bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conveniance funcstions\n",
    "def build_dataloaders(\n",
    "    root: os.PathLike,\n",
    "    gesture_names: Sequence[str],\n",
    "    *,\n",
    "    batch_size: int = 16,\n",
    "    range_bins: int = 64,\n",
    "    split: float = 0.8,\n",
    "    seed: int = 1,\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    ds = RadarDataset(root, gesture_names, range_bins=range_bins)\n",
    "    n_train = int(len(ds) * split)\n",
    "    n_val = len(ds) - n_train\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    train_ds, val_ds = random_split(ds, [n_train, n_val], generator=g)\n",
    "\n",
    "    pin = torch.cuda.is_available()\n",
    "    train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=0, pin_memory=pin)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size, shuffle=False, num_workers=0, pin_memory=pin)\n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "def inspect_dataset(ds: RadarDataset) -> None:\n",
    "    table = []\n",
    "    for g in ds.gesture_names:\n",
    "        clips = [p for p, lbl in ds.index if lbl == ds.g2idx[g]]\n",
    "        sample = ds[random.choice(range(len(clips)))]  # (x,label)\n",
    "        table.append((g, len(clips), tuple(sample[0].shape), sample[0].min().item(), sample[0].max().item()))\n",
    "    header = (\"Gesture\", \"#clips\", \"shape(T,R)\", \"min\", \"max\")\n",
    "    colw = [max(len(str(x)) for x in col) for col in zip(header, *table)]\n",
    "    def row(r):\n",
    "        return \"  \".join(str(x).ljust(w) for x, w in zip(r, colw))\n",
    "    print(row(header))\n",
    "    for r in table:\n",
    "        print(row(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8fb3f",
   "metadata": {},
   "source": [
    "# Loading Data 5G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b73d64d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No clips found under C:\\Users\\ioana\\Documents\\VS Projects\\ML-on-Microcontrollers-Project-Ioana-Gidiuta\\Datasets\\5G\\data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m GESTURES \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFingerSlider\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoHand\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPalmTilt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPullUp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPushDown\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSwipeRL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxNoGesture\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1) loaders\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_dl, val_dl \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataloaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mROOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgesture_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGESTURES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrange_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# stays 64 for your .txt format\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 2) quick sanity check on GPU\u001b[39;00m\n\u001b[0;32m     14\u001b[0m ap \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mArgumentParser(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyRadarNN dataset sanity‑check\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 11\u001b[0m, in \u001b[0;36mbuild_dataloaders\u001b[1;34m(root, gesture_names, batch_size, range_bins, split, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_dataloaders\u001b[39m(\n\u001b[0;32m      3\u001b[0m     root: os\u001b[38;5;241m.\u001b[39mPathLike,\n\u001b[0;32m      4\u001b[0m     gesture_names: Sequence[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     10\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[DataLoader, DataLoader]:\n\u001b[1;32m---> 11\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mRadarDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgesture_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrange_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrange_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     n_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ds) \u001b[38;5;241m*\u001b[39m split)\n\u001b[0;32m     13\u001b[0m     n_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ds) \u001b[38;5;241m-\u001b[39m n_train\n",
      "Cell \u001b[1;32mIn[41], line 19\u001b[0m, in \u001b[0;36mRadarDataset.__init__\u001b[1;34m(self, root, gesture_names, range_bins, file_type)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_type \u001b[38;5;241m=\u001b[39m file_type  \u001b[38;5;66;03m# \"txt\", \"npy\", \"png\" or None → auto\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex: List[Tuple[Path, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# (path, label)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 46\u001b[0m, in \u001b[0;36mRadarDataset._build_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mappend((p, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg2idx[g]))\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo clips found under \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Auto‑detect file type if user did not specify one\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msuffix[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No clips found under C:\\Users\\ioana\\Documents\\VS Projects\\ML-on-Microcontrollers-Project-Ioana-Gidiuta\\Datasets\\5G\\data"
     ]
    }
   ],
   "source": [
    "ROOT = Path(r\"C:\\Users\\ioana\\Documents\\VS Projects\\ML-on-Microcontrollers-Project-Ioana-Gidiuta\\Datasets\\5G\\data\")\n",
    "GESTURES = [\"FingerSlider\",\"NoHand\",\"PalmTilt\",\"PullUp\",\"PushDown\",\"SwipeRL\",\"xNoGesture\"]\n",
    "\n",
    "# 1) loaders\n",
    "train_dl, val_dl = build_dataloaders(\n",
    "    ROOT,\n",
    "    gesture_names=GESTURES,\n",
    "    batch_size=32,\n",
    "    range_bins=64,        # stays 64 for your .txt format\n",
    ")\n",
    "\n",
    "# 2) quick sanity check on GPU\n",
    "\n",
    "ap = argparse.ArgumentParser(description=\"TinyRadarNN dataset sanity‑check\")\n",
    "ap.add_argument(\"root\", type=Path, help=\"Path to 5G or 11G data folder\")\n",
    "ap.add_argument(\"--range_bins\", type=int, default=64, help=\"# range bins per sweep (for .txt)\")\n",
    "ap.add_argument(\"--gesture\", action=\"append\", required=True, help=\"Gesture names to include (repeatable)\")\n",
    "args = ap.parse_args()\n",
    "\n",
    "ds = RadarDataset(args.root, args.gesture, range_bins=args.range_bins)\n",
    "print(f\"Loaded {len(ds)} clips across {len(ds.gesture_names)} classes: {ds.gesture_names}\")\n",
    "inspect_dataset(ds)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1130a278",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[0;32m      2\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), y\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m( x\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, y[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\ioana\\anaconda3\\envs\\tinygesture\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ioana\\anaconda3\\envs\\tinygesture\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ioana\\anaconda3\\envs\\tinygesture\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ioana\\anaconda3\\envs\\tinygesture\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ioana\\anaconda3\\envs\\tinygesture\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ioana\\anaconda3\\envs\\tinygesture\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ioana\\anaconda3\\envs\\tinygesture\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    236\u001b[0m                 collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    237\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    238\u001b[0m             ]\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "for x, y in train_dl:\n",
    "    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "    print( x.shape, \"labels\", y[:5].tolist())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e77eae",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7e30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinygesture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
